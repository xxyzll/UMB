{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285e5ac-e4c6-46f6-8b8c-71378a8580af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Modified from PROB: Probabilistic Objectness for Open World Object Detection\n",
    "# Orr Zohar, Jackson Wang, Serena Yeung\n",
    "# ------------------------------------------------------------------------\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import util.misc as utils\n",
    "import datasets.samplers as samplers\n",
    "from datasets import build_dataset\n",
    "from models import build_model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a692af3-a7fc-4416-abe7-93358aa21455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('RWD - FOMO Detector', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=10, type=int)\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--output_dir', default='tmp/rwod',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--viz', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "\n",
    "    \n",
    "    ################ RWOD ################\n",
    "    parser.add_argument('--test_set', default='test.txt', help='testing txt files')\n",
    "    parser.add_argument('--train_set', default='train.txt', help='training txt files')\n",
    "\n",
    "    parser.add_argument('--dataset', default='?', help='defines which dataset is used.')\n",
    "    parser.add_argument('--data_root', default='./data', type=str)\n",
    "    parser.add_argument('--data_task', default='RWOD', type=str)\n",
    "    parser.add_argument('--unknown_classnames_file', default='', type=str)\n",
    "    parser.add_argument('--classnames_file', default='known_classnames.txt', type=str)\n",
    "    parser.add_argument('--prev_classnames_file', default='known_classnames.txt', type=str)\n",
    "    parser.add_argument('--templates_file', default='best_templates.txt', type=str)\n",
    "    parser.add_argument('--attributes_file', default='attributes1.json', type=str)\n",
    "    parser.add_argument('--use_attributes', action='store_true')\n",
    "    parser.add_argument('--att_selection', action='store_true')\n",
    "    parser.add_argument('--image_conditioned_file', default='few_shot_data.json', type=str)\n",
    "\n",
    "    parser.add_argument('--image_conditioned', action='store_true')\n",
    "    parser.add_argument('--att_refinement',  action='store_true')\n",
    "    parser.add_argument('--att_adapt',  action='store_true')\n",
    "    parser.add_argument('--unk_proposal',  action='store_true')\n",
    "\n",
    "    parser.add_argument('--num_few_shot', default=100, type=int)\n",
    "    parser.add_argument('--num_att_per_class', default=25, type=int)\n",
    "\n",
    "    # model config\n",
    "    parser.add_argument('--unk_methods', default='sigmoid-max-mcm', type=str)#,sigmoid-max\n",
    "    parser.add_argument('--unk_method', default='sigmoid-max-mcm', type=str)\n",
    "    parser.add_argument('--model_type', default='owl_vit', type=str)\n",
    "    parser.add_argument('--model_name', default='google/owlvit-base-patch16', type=str)\n",
    "    parser.add_argument('--unk_LLM', action='store_true')\n",
    "    parser.add_argument('--image_resize', default=768, type=int,\n",
    "                        help='image resize 768 for owlvit-base models, 840 for owlvit-large models')\n",
    "    parser.add_argument('--pred_per_im', default=100, type=int)\n",
    "    parser.add_argument('--PREV_INTRODUCED_CLS', default=0, type=int)\n",
    "    parser.add_argument('--CUR_INTRODUCED_CLS', default=30, type=int)\n",
    "    parser.add_argument('--prev_output_file', default='', type=str)\n",
    "    parser.add_argument('--output_file', default='', type=str)\n",
    "    # logging\n",
    "    parser.add_argument('--TCP', default='295499', type=str)\n",
    "\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f85c7-6c00-4c7e-82bb-9a31a8b436b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloader(args, dataset, train=True):\n",
    "    if args.distributed:\n",
    "        sampler = samplers.DistributedSampler(dataset, shuffle=train)\n",
    "    else:\n",
    "        if train:\n",
    "            sampler = torch.utils.data.RandomSampler(dataset)\n",
    "        else:\n",
    "            sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "\n",
    "    if train:\n",
    "        batch_sampler = torch.utils.data.BatchSampler(sampler, args.batch_size, drop_last=True)\n",
    "        data_loader = DataLoader(dataset, batch_sampler=batch_sampler,\n",
    "                                 collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                 pin_memory=True)\n",
    "    else:\n",
    "        data_loader = DataLoader(dataset, args.batch_size, sampler=sampler,\n",
    "                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                 pin_memory=True)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec8e71-da05-4132-810e-0692a6773332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset=\"DIOR_FIN\"\n",
    "\n",
    "if dataset==\"AQUA\":\n",
    "    args_str = '--model_name google/owlvit-base-patch16 --num_few_shot 100 --batch_size 6 ' \\\n",
    "               '--PREV_INTRODUCED_CLS 0 --CUR_INTRODUCED_CLS 4 --TCP 29550 ' \\\n",
    "               '--dataset AQUA --image_conditioned --image_resize 768 ' \\\n",
    "               '--att_adapt --att_selection --att_refinement '\n",
    "\n",
    "elif dataset == \"DIOR_FIN\":\n",
    "    args_str = '--model_name google/owlvit-base-patch16 --num_few_shot 100 --batch_size 6 ' \\\n",
    "               '--PREV_INTRODUCED_CLS 0 --CUR_INTRODUCED_CLS 10 --TCP 29550 ' \\\n",
    "               '--dataset DIOR_FIN --image_conditioned --image_resize 768 ' \\\n",
    "               '--att_adapt --att_selection --att_refinement '\n",
    "    \n",
    "elif dataset == \"NEUROSURGICAL_TOOLS_FIN\":\n",
    "    args_str = '--model_name google/owlvit-base-patch16 --num_few_shot 100 --batch_size 6 ' \\\n",
    "               '--PREV_INTRODUCED_CLS 0 --CUR_INTRODUCED_CLS 6 --TCP 29550 ' \\\n",
    "               '--dataset NEUROSURGICAL_TOOLS_FIN --image_conditioned --image_resize 768 ' \\\n",
    "               '--att_adapt --att_selection --att_refinement '\n",
    "    \n",
    "elif dataset == \"XRAY\":\n",
    "    args_str = '--model_name google/owlvit-base-patch16 --num_few_shot 100 --batch_size 6 ' \\\n",
    "               '--PREV_INTRODUCED_CLS 0 --CUR_INTRODUCED_CLS 6 --TCP 29550 ' \\\n",
    "               '--dataset XRAY --image_conditioned --image_resize 768 ' \\\n",
    "               '--att_adapt --att_selection --att_refinement '\n",
    "    \n",
    "elif dataset == \"SYNTH\":\n",
    "    args_str = '--model_name google/owlvit-base-patch16 --num_few_shot 100 --batch_size 6 ' \\\n",
    "               '--PREV_INTRODUCED_CLS 0 --CUR_INTRODUCED_CLS 30 --TCP 29550 ' \\\n",
    "               '--dataset SYNTH --image_conditioned --image_resize 768 ' \\\n",
    "               '--att_adapt --att_selection --att_refinement '\n",
    "    \n",
    "# Split the arguments string into a list of arguments\n",
    "# It's important to note that each space-separated element is a separate item in the list\n",
    "args_list = args_str.split()\n",
    "parser = get_args_parser()\n",
    "# Now parse the arguments list\n",
    "args = parser.parse_args(args=args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc072247-0b7f-4ae3-9044-a11531e9a1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(args)\n",
    "utils.init_distributed_mode(args)\n",
    "print(\"git:\\n  {}\\n\".format(utils.get_sha()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83698f57-60b7-4d08-a116-9d9bdd3fd759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = args.seed + utils.get_rank()\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "###### get datasets ######\n",
    "if len(args.train_set) > 0:\n",
    "    dataset_train = build_dataset(args, args.train_set)\n",
    "    data_loader_train = get_dataloader(args, dataset_train, train=False)\n",
    "\n",
    "if len(args.test_set) > 0:\n",
    "    dataset_val = build_dataset(args, args.test_set)\n",
    "    data_loader_val = get_dataloader(args, dataset_val, train=False)\n",
    "args.neg_sup_ep = 1\n",
    "args.neg_sup_lr = 5e-05\n",
    "\n",
    "model, postprocessors = build_model(args)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda62e69-019d-4961-b929-def96c132ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_method = args.unk_methods.split(\",\")[0]\n",
    "model.unk_head.method = unk_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc260d6-d0a4-4d01-8785-be2d3ccb96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.open_world_eval_attributes import OWEvaluator\n",
    "from collections import defaultdict\n",
    "\n",
    "att_W = model.unk_head.att_W\n",
    "num_att = att_W.shape[0]\n",
    "# Initialize the sum and count dictionaries\n",
    "attribute_sums = defaultdict(lambda: np.zeros(num_att))  # Assuming there are 96 attributes\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "with torch.no_grad():\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "    coco_evaluator = OWEvaluator(dataset_val, args=args)\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    for samples, targets in metric_logger.log_every(data_loader_val, 10, header):\n",
    "        samples = samples.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        outputs = model(samples.tensors)\n",
    "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "        results = postprocessors(outputs, orig_target_sizes)\n",
    "        for result in results:\n",
    "            labels = result['labels']\n",
    "            attributes = result['attributes']\n",
    "            for label, attribute in zip(labels, attributes):\n",
    "                lab = label.item()  \n",
    "                att = attribute.detach().cpu().numpy() \n",
    "                attribute_sums[lab] += att# Accumulate the attribute scores\n",
    "                class_counts[lab] += 1  # Count the images per class\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f150b-339a-4a5a-8746-89bed4e6effa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate average attributes per class ensuring tensors are moved to CPU\n",
    "average_attributes_per_class = {cls: (attribute_sums[cls] / count) for cls, count in class_counts.items() if count > 0}\n",
    "\n",
    "# Now create the DataFrame\n",
    "average_attributes_df = pd.DataFrame({k: v for k, v in average_attributes_per_class.items()}).T.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040623b-01c8-4680-8c3f-604132b657f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to a DataFrame\n",
    "average_attributes_df.columns = model.attributes_texts + [0]  # Rename columns\n",
    "\n",
    "# Add class names if you have them\n",
    "class_names = {i:model.all_classnames[i] for i in range(len(model.all_classnames))}\n",
    "average_attributes_df.rename(index=class_names, inplace=True)\n",
    "if False:\n",
    "    att_W[:,-1] = 1\n",
    "    average_attributes_df = average_attributes_df * att_W.detach().cpu().numpy().T\n",
    "average_attributes_df = average_attributes_df.iloc[:, :-1]  # This selects all rows and all columns except the last one\n",
    "average_attributes_df = average_attributes_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ebc2b-aaf1-4547-8dbd-af7c488e6914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_att=[]\n",
    "for i in range(len(average_attributes_df)-1):\n",
    "    top_att.append(average_attributes_df.iloc[i].idxmax())\n",
    "top_att=list(set(top_att))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93668f2f-5cc2-4da4-a027-646e8857d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = average_attributes_df[top_att].T\n",
    "selected_unused_attributes = np.random.choice(model.unused_attributes_texts, size=2, replace=False)\n",
    "\n",
    "unused_attributes_df = pd.DataFrame(0, index=selected_unused_attributes, columns=df.columns)\n",
    "df_with_unused = pd.concat([df, unused_attributes_df])\n",
    "df = df_with_unused.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fef4fa-531d-4597-8775-1ffd94a13c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = df.columns\n",
    "classnames = list(df.index)\n",
    "classnames[-1]=\"unknown\"\n",
    "fig, ax = plt.subplots(figsize=(20*len(classnames)/31+2, 4*len(categories)/6-3))\n",
    "sns.heatmap(df.T, annot=True, fmt=\".2f\", linewidths=.7, cmap='coolwarm', ax=ax)\n",
    "edited_categories = [cat.replace(\"object which (is/has/etc) \", \"\") for cat in categories]\n",
    "ax.set_yticklabels(edited_categories)\n",
    "ax.set_xticklabels(classnames, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{dataset}_att.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea7f7a8-9434-4035-ba4f-ff69a6ce1460",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.columns\n",
    "classnames = list(df.index)\n",
    "classnames[-1]=\"unknown\"\n",
    "fig, ax = plt.subplots(figsize=(5, 2))\n",
    "sns.heatmap(df, annot=True, fmt=\".2f\", linewidths=.5, cmap='coolwarm', ax=ax)\n",
    "edited_categories = [cat.replace(\"object which (is/has/etc) \", \"\") for cat in categories]\n",
    "ax.set_xticklabels(edited_categories,rotation=45,ha='right')\n",
    "ax.set_yticklabels(classnames)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
